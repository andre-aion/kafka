{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-16b9219b1fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Poolminers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import mysql\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "import logging\n",
    "from cassandra.cqlengine import columns\n",
    "from cassandra.cqlengine.models import Model\n",
    "from cassandra.cluster import Cluster, BatchStatement\n",
    "from cassandra.query import SimpleStatement\n",
    "\n",
    "from kafka.client import KafkaClient\n",
    "from kafka.consumer import SimpleConsumer\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pdb import set_trace\n",
    "\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark/spark-2.3.2-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Poolminers\") \\\n",
    "    .getOrCreate()\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "\n",
    "class PythonCassandra:\n",
    "    def __init__(self):\n",
    "        self.cluster = None\n",
    "        self.session = None\n",
    "        self.keyspace = None\n",
    "        self.log = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cluster.shutdown()\n",
    "\n",
    "    def createsession(self):\n",
    "        self.cluster = Cluster(['localhost'])\n",
    "        self.session = self.cluster.connect(self.keyspace)\n",
    "\n",
    "    def getsession(self):\n",
    "        return self.session\n",
    "\n",
    "    # How about Adding some log info to see what went wrong\n",
    "    def setlogger(self):\n",
    "        log = logging.getLogger()\n",
    "        log.setLevel('INFO')\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"))\n",
    "        log.addHandler(handler)\n",
    "        self.log = log\n",
    "\n",
    "    # Create Keyspace based on Given Name\n",
    "    def createkeyspace(self, keyspace):\n",
    "        \"\"\"\n",
    "        :param keyspace:  The Name of Keyspace to be created\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Before we create new lets check if exiting keyspace; we will drop that and create new\n",
    "        rows = self.session.execute(\"SELECT keyspace_name FROM system_schema.keyspaces\")\n",
    "        keyspace_exists = False\n",
    "        if keyspace in [row[0] for row in rows]:\n",
    "            keyspace_exists = True\n",
    "        if keyspace_exists is False:\n",
    "            # self.log.info(\"dropping existing keyspace...\")\n",
    "            # self.session.execute(\"DROP KEYSPACE \" + keyspace)\n",
    "            self.log.info(\"creating keyspace...\")\n",
    "            self.session.execute(\"\"\"\n",
    "                CREATE KEYSPACE %s\n",
    "                WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': '1' }\n",
    "                \"\"\" % keyspace)\n",
    "            self.log.info(\"setting keyspace...\")\n",
    "        self.session.set_keyspace(keyspace)\n",
    "\n",
    "    def create_table_block(self):\n",
    "        c_sql = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS block (block_number bigint, block_hash varchar,\n",
    "                                              miner_address varchar, parent_hash varchar, receipt_tx_root varchar,\n",
    "                                              state_root varchar, tx_trie_root varchar, extra_data varchar, \n",
    "                                              nonce varchar, bloom varchar, solution varchar, difficulty varchar, \n",
    "                                              total_difficulty varchar, nrg_consumed bigint, nrg_limit bigint,\n",
    "                                              block_size bigint, block_timestamp bigint, num_transactions bigint,\n",
    "                                              block_time bigint, nrg_reward varchar, transaction_id bigint,\n",
    "                                              transaction_list varchar,\n",
    "                                              PRIMARY KEY (block_timestamp,block_number)\n",
    "                                              );\n",
    "                 \"\"\"\n",
    "        self.session.execute(c_sql)\n",
    "        self.log.info(\"Block Table Created !!!\")\n",
    "\n",
    "    def create_table_transaction(self):\n",
    "        c_sql = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS transaction (id bigint,\n",
    "                                              transaction_hash varchar, block_hash varchar, block_number bigint,\n",
    "                                              transaction_index bigint, from_addr varchar, to_addr varchar, \n",
    "                                              nrg_consumed bigint, nrg_price bigint, transaction_timestamp bigint,\n",
    "                                              block_timestamp bigint, tx_value varchar, transaction_log varchar,\n",
    "                                              tx_data varchar, nonce varchar, tx_error varchar, contract_addr varchar,\n",
    "                                              PRIMARY KEY (block_timestamp,block_number,transaction_timestamp)\n",
    "                                              );\n",
    "                 \"\"\"\n",
    "        self.session.execute(c_sql)\n",
    "        self.log.info(\"Transaction Table Created !!!\")\n",
    "\n",
    "    def insert_data_transaction(self, message):\n",
    "        insert_sql = self.session.prepare(\n",
    "            \"\"\" INSERT INTO transaction(\n",
    "            id,transaction_hash, block_hash, block_number,\n",
    "            transaction_index, from_addr, to_addr, \n",
    "            nrg_consumed, nrg_price, transaction_timestamp,\n",
    "            block_timestamp, tx_value, transaction_log,\n",
    "            tx_data, nonce, tx_error, contract_addr)\n",
    "            VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "            \"\"\"\n",
    "        )\n",
    "        batch = BatchStatement()\n",
    "        # batch.add(insert_sql, (1, 'LyubovK'))\n",
    "        batch.add(insert_sql, message)\n",
    "        self.session.execute(batch)\n",
    "        self.log.info('Batch Insert Completed')\n",
    "\n",
    "    def insert_data_block(self, message):\n",
    "        insert_sql = self.session.prepare(\"\"\"\n",
    "                                            INSERT INTO block(block_number, block_hash, miner_address, \n",
    "                                            parent_hash, receipt_tx_root,\n",
    "                                            state_root, tx_trie_root, extra_data, \n",
    "                                            nonce, bloom, solution, difficulty, \n",
    "                                            total_difficulty, nrg_consumed, nrg_limit,\n",
    "                                            block_size, block_timestamp, num_transactions,\n",
    "                                            block_time, nrg_reward, transaction_id, transaction_list) \n",
    "                                            VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "                                            \"\"\")\n",
    "        batch = BatchStatement()\n",
    "        # batch.add(insert_sql, (1, 'LyubovK'))\n",
    "        batch.add(insert_sql, message)\n",
    "\n",
    "        self.session.execute(batch)\n",
    "        self.log.info('Block Batch Insert Completed')\n",
    "\n",
    "    def select_data(self, table):\n",
    "        rows = self.session.execute('select * from ' + table)\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "    def update_data(self):\n",
    "        pass\n",
    "\n",
    "    def delete_data(self):\n",
    "        pass\n",
    "\n",
    "class KafkaConnectPyspark:\n",
    "    def __init__(self):\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar pyspark-shell'\n",
    "        self.sc = SparkContext.getOrCreate(spark)\n",
    "        self.ssc = StreamingContext(self.sc,60) #\n",
    "        self.kafkaStream=None\n",
    "\n",
    "    def connect(self):\n",
    "        #broker, topic = sys.argv[1:]\n",
    "        broker='localhost:2181'\n",
    "        topic='aionv4server.aionv4.block'\n",
    "        self.kafkaStream = KafkaUtils.createStream(self.ssc,broker,\n",
    "                                                   'block-streaming-consumer', {topic: 1})\n",
    "\n",
    "        rows = self.kafkaStream.map(lambda x: x[1])\n",
    "        rows.pprint()\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KafkaConnectPyspark' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ecb2a135c98d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaConnectPyspark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkafkaStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KafkaConnectPyspark' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "kcp = KafkaConnectPyspark()\n",
    "kcp.connect()\n",
    "kcp.kafkaStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'CONDA_PYTHON_EXE': '/home/andre/anaconda3/bin/python', 'GNOME_SHELL_SESSION_MODE': 'ubuntu', 'LC_NAME': 'es_PR.UTF-8', 'XDG_SESSION_TYPE': 'x11', 'GPG_AGENT_INFO': '/run/user/1000/gnupg/S.gpg-agent:0:1', 'QT_IM_MODULE': 'ibus', 'GDMSESSION': 'ubuntu', 'GTK2_MODULES': 'overlay-scrollbar', 'PROJ_LIB': '/home/andre/anaconda3/envs/bokeh_aion_analytics/share/proj', 'XMODIFIERS': '@im=ibus', 'SDKMAN_CANDIDATES_DIR': '/home/andre/.sdkman/candidates', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/andre/anaconda3/envs/bokeh_aion_analytics/bin/jupyter', 'QT_ACCESSIBILITY': '1', 'GRADLE_HOME': '/home/andre/.sdkman/candidates/gradle/current', 'KAFKA_HOME': '/home/kafka/kafka', 'SPARK_HOME': '/usr/local/spark/spark-2.3.2-bin-hadoop2.7', 'XDG_MENU_PREFIX': 'gnome-', '_CONDA_SET_PROJ_LIB': 'PROJ_LIB', 'XDG_CURRENT_DESKTOP': 'ubuntu:GNOME', 'TEXTDOMAIN': 'im-config', 'XDG_VTNR': '2', 'PAGER': 'cat', 'DISPLAY': ':0', 'HOME': '/home/andre', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus', 'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated', 'PATH': '/home/andre/anaconda3/envs/bokeh_aion_analytics/bin:/home/andre/anaconda3/envs/bokeh_aion_analytics/bin:/home/andre/.sdkman/candidates/gradle/current/bin:/usr/local/cassandra/bin:/home/kafka/kafka/bin:/usr/local/spark/spark-2.3.2-bin-hadoop2.7/bin:/home/andre/anaconda3/bin:/home/andre/bin:/home/andre/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/andre/.dotnet/tools', 'JPY_PARENT_PID': '29130', 'S_COLORS': 'auto', 'COLORTERM': 'truecolor', 'PYSPARK_SUBMIT_ARGS': '--packages org.apache.spark:spark-streaming-kafka-0-8:2.3.2 pyspark-shell', 'GNOME_TERMINAL_SCREEN': '/org/gnome/Terminal/screen/af2c5f79_2d42_4f88_a9d8_c54b4cde44e0', 'CONDA_PROMPT_MODIFIER': '(bokeh_aion_analytics) ', 'USERNAME': 'andre', 'USER': 'andre', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'CONDA_PREFIX': '/home/andre/anaconda3/envs/bokeh_aion_analytics', 'LC_MEASUREMENT': 'es_PR.UTF-8', 'GJS_DEBUG_TOPICS': 'JS ERROR;JS LOG', 'DESKTOP_SESSION': 'ubuntu', 'PYSPARK_PYTHON': '/home/andre/anaconda3/envs/bokeh_aion_analytics/bin/python', 'CONDA_DEFAULT_ENV': 'bokeh_aion_analytics', 'XDG_SEAT': 'seat0', 'GDAL_DATA': '/home/andre/anaconda3/envs/bokeh_aion_analytics/etc/conda/activate.d/../../../share/gdal', 'PWD': '/home/andre/aion/data_science', 'TEXTDOMAINDIR': '/usr/share/locale/', 'LC_TIME': 'es_PR.UTF-8', 'SDKMAN_DIR': '/home/andre/.sdkman', 'LC_PAPER': 'es_PR.UTF-8', 'XDG_DATA_DIRS': '/usr/share/ubuntu:/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'LC_NUMERIC': 'es_PR.UTF-8', 'CASSANDRA_HOME': '/usr/local/cassandra', 'LC_TELEPHONE': 'es_PR.UTF-8', 'LC_MONETARY': 'es_PR.UTF-8', 'XDG_SESSION_DESKTOP': 'ubuntu', 'XDG_SESSION_ID': '5', 'JDK_10': '/opt/bin/jdk-10.0.2', 'LC_ADDRESS': 'es_PR.UTF-8', 'GJS_DEBUG_OUTPUT': 'stderr', 'LANG': 'en_US.UTF-8', 'OLDPWD': '/home/andre', 'DEFAULTS_PATH': '/usr/share/gconf/ubuntu.default.path', 'SDKMAN_VERSION': '5.7.2+323', 'SDKMAN_CANDIDATES_API': 'https://api.sdkman.io/2', 'SSH_AUTH_SOCK': '/run/user/1000/keyring/ssh', 'CLUTTER_IM_MODULE': 'xim', 'MANDATORY_PATH': '/usr/share/gconf/ubuntu.mandatory.path', 'QT_QPA_PLATFORMTHEME': 'appmenu-qt5', 'SHELL': '/bin/bash', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'LOGNAME': 'andre', '_CONDA_SET_GDAL_DATA': '1', 'GIT_PAGER': 'cat', 'GNOME_TERMINAL_SERVICE': ':1.150', 'XAUTHORITY': '/run/user/1000/gdm/Xauthority', 'SHLVL': '1', 'SSH_AGENT_PID': '6783', 'TERM': 'xterm-color', 'GTK_MODULES': 'gail:atk-bridge', 'XDG_CONFIG_DIRS': '/etc/xdg/xdg-ubuntu:/etc/xdg', 'IM_CONFIG_PHASE': '2', 'QT4_IM_MODULE': 'xim', 'LC_IDENTIFICATION': 'es_PR.UTF-8', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'SDKMAN_PLATFORM': 'Linux64', 'WINDOWPATH': '2', 'CONDA_SHLVL': '1', 'VTE_VERSION': '5202', 'GTK_IM_MODULE': 'ibus', 'CONDA_EXE': '/home/andre/anaconda3/bin/conda', 'SESSION_MANAGER': 'local/andre-HP-ProBook-450-G5:@/tmp/.ICE-unix/6679,unix/andre-HP-ProBook-450-G5:/tmp/.ICE-unix/6679', 'CLICOLOR': '1', 'XDG_RUNTIME_DIR': '/run/user/1000'})\n"
     ]
    }
   ],
   "source": [
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kcp' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e9f1ee826798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kcp' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "kcp.sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
